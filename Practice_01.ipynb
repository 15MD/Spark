{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','sal','mngr_id']\n",
    "\n",
    "emp_df= spark.createDataFrame(data=data,schema=schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "location = ['Pune','Nagar','Mumbai','Solapur','Kolhapur','Satara','Nashik','Jamkhed','Pimpalgaon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# emp_df.withColumn('Location', F.expr('F.lit(i) for i in location')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----+\n",
      "|  Name|     Ref_Id|  sal|\n",
      "+------+-----------+-----+\n",
      "|Mahesh|DIV-CHN_099|50000|\n",
      "|  bala| DIV-PN_999|75000|\n",
      "|  Sonu|DIV-MUM_799|40000|\n",
      "|  Monu|DIV-ANG_769|60000|\n",
      "+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('Mahesh' ,'DIV-CHN_099',50000),\n",
    "('bala' ,'DIV-PN_999',75000),\n",
    "('Sonu' ,'DIV-MUM_799',40000),\n",
    "('Monu' ,'DIV-ANG_769',60000),\n",
    "]\n",
    "\n",
    "schema=['Name','Ref_Id','sal']\n",
    "\n",
    "emp_df1= spark.createDataFrame(data=data,schema=schema)\n",
    "emp_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = emp_df1.withColumn('Location',split(col('Ref_Id'),'-')[1])\n",
    "# new_df.withColumn('Location',split(col('Location'),'_')[0]).show()\n",
    "\n",
    "emp_df2 =emp_df1.withColumn('Location',split(split(col('Ref_Id'),'-')[1],'_')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----+-----------+\n",
      "|  Name|     Ref_Id|  sal|   Location|\n",
      "+------+-----------+-----+-----------+\n",
      "|Mahesh|DIV-CHN_099|50000|    Chennai|\n",
      "|  bala| DIV-PN_999|75000|       Pune|\n",
      "|  Sonu|DIV-MUM_799|40000|     Mumbai|\n",
      "|  Monu|DIV-ANG_769|60000|Ahmendnagar|\n",
      "+------+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.withColumn('Location', when(emp_df2.Location=='CHN','Chennai')\n",
    " .when(col('Location')=='PN','Pune')\n",
    " .when(col('Location')=='MUM','Mumbai')\n",
    " .when(col('Location')=='ANG','Ahmendnagar').otherwise('NULL')).show()\n",
    "                       \n",
    "# emp_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----+\n",
      "|  Name|     Ref_Id|  sal|\n",
      "+------+-----------+-----+\n",
      "|Mahesh|DIV-CHN_099|50000|\n",
      "|  bala| DIV-PN_999|75000|\n",
      "|  Sonu|DIV-MUM_799|40000|\n",
      "|  Monu|DIV-ANG_769|60000|\n",
      "+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+\n",
      "| id|  name|  sal| Dept|\n",
      "+---+------+-----+-----+\n",
      "| 10|  Anil|50000| Auto|\n",
      "| 11| Vikas|75000| Auto|\n",
      "| 12| Nisha|40000|Civil|\n",
      "| 13| Nidhi|60000| Auto|\n",
      "| 14| Priya|80000|Civil|\n",
      "| 15| Mohit|45000| Auto|\n",
      "| 16|Rajesh|90000| Mech|\n",
      "| 17| Raman|55000| Mech|\n",
      "| 18|   Sam|65000|Civil|\n",
      "| 11| Sadhu|75000| Auto|\n",
      "+---+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, \"Auto\"),\n",
    "(11 ,'Vikas',75000,  \"Auto\"),\n",
    "(12 ,'Nisha',40000,  \"Civil\"),\n",
    "(13 ,'Nidhi',60000,  \"Auto\"),\n",
    "(14 ,'Priya',80000,  \"Civil\"),\n",
    "(15 ,'Mohit',45000,  \"Auto\"),\n",
    "(16 ,'Rajesh',90000, \"Mech\"),\n",
    "(17 ,'Raman',55000, \"Mech\"),\n",
    "(18 ,'Sam',65000,   \"Civil\"),\n",
    "(11 ,'Sadhu',75000,  \"Auto\")]\n",
    "\n",
    "schema=['id','name','sal','Dept']\n",
    "\n",
    "employe_df = spark.createDataFrame(data=data,schema=schema)\n",
    "employe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "employe_df.createOrReplaceTempView('emp_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|max(sal)| Dept|\n",
      "+--------+-----+\n",
      "|   75000| Auto|\n",
      "|   80000|Civil|\n",
      "|   90000| Mech|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "              SELECT max(sal), Dept FROM emp_tble\n",
    "              GROUP BY Dept\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|max(sal)|  name|\n",
      "+--------+------+\n",
      "|   50000|  Anil|\n",
      "|   75000| Vikas|\n",
      "|   40000| Nisha|\n",
      "|   60000| Nidhi|\n",
      "|   80000| Priya|\n",
      "|   45000| Mohit|\n",
      "|   90000|Rajesh|\n",
      "|   65000|   Sam|\n",
      "|   55000| Raman|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "              SELECT max(sal),name FROM emp_tble\n",
    "              GROUP BY name\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('Dept').orderBy(col('Sal').desc())\n",
    "\n",
    "employe_df_new = employe_df.withColumn('Rank',dense_rank().over(window)).filter(col('Rank')<2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  Sal| Dept|\n",
      "+-----+-----+\n",
      "|75000| Auto|\n",
      "|80000|Civil|\n",
      "|90000| Mech|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df_new.select('Sal','Dept').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "| id| name|  sal|DeptId|\n",
      "+---+-----+-----+------+\n",
      "|  1|  Joe|70000|     1|\n",
      "|  2|  Jim|90000|     1|\n",
      "|  3|Henry|80000|     2|\n",
      "|  4|  Sam|60000|     2|\n",
      "|  5|  Max|90000|     1|\n",
      "+---+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(sal)|\n",
      "+--------+\n",
      "|  390000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.select(sum('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "employe_df.createOrReplaceTempView('employe_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "| id| name|  sal|DeptId|\n",
      "+---+-----+-----+------+\n",
      "|  1|  Joe|70000|     1|\n",
      "|  2|  Jim|90000|     1|\n",
      "|  3|Henry|80000|     2|\n",
      "|  4|  Sam|60000|     2|\n",
      "|  5|  Max|90000|     1|\n",
      "+---+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|DeptId|max(sal)|\n",
      "+------+--------+\n",
      "|     1|   90000|\n",
      "|     2|   80000|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = employe_df.groupBy('DeptId').agg(max('sal'))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+------+--------+\n",
      "| id| name|  sal|DeptId|DeptId|max(sal)|\n",
      "+---+-----+-----+------+------+--------+\n",
      "|  2|  Jim|90000|     1|     1|   90000|\n",
      "|  3|Henry|80000|     2|     2|   80000|\n",
      "|  5|  Max|90000|     1|     1|   90000|\n",
      "+---+-----+-----+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df= employe_df.alias('emp').join(new_df.alias('ndf'),(col('emp.DeptId')== col('ndf.DeptId')) & (col('emp.sal')== col('ndf.max(sal)')) ,'inner')\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id|DeptId|max(sal)|\n",
      "+---+------+--------+\n",
      "|  1|     1|   70000|\n",
      "|  2|     1|   90000|\n",
      "|  3|     2|   80000|\n",
      "|  4|     2|   60000|\n",
      "|  5|     1|   90000|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "            select id,DeptId,max(sal) from employe_tble\n",
    "            group by DeptId,id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|DeptId|max(sal)|\n",
      "+------+--------+\n",
      "|     1|   90000|\n",
      "|     2|   80000|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "            select id,DeptId,max(sal) from employe_tble\n",
    "            group by DeptId\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+\n",
      "|sum(id)|sum(sal)|sum(DeptId)|\n",
      "+-------+--------+-----------+\n",
      "|     15|  390000|          7|\n",
      "+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employe_df.agg(sum(\"sal\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(sal)=390000)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employe_df.agg(sum(\"sal\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sum(sal)=390000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employe_df.agg(sum(\"sal\")).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(sal)|\n",
      "+--------+\n",
      "|  390000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employe_df.agg(sum(\"sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "window= Window.orderBy(col('sal').desc())\n",
    "\n",
    "new_df= employe_df.withColumn('Dense_rank',dense_rank().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.withColumn('Dense_rank',when('Dense_rank' == 2,(filter('Dense_rank' ==2 )) ).otherwise('NULL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+----------+\n",
      "| id| name|  sal| Dept|Dense_rank|\n",
      "+---+-----+-----+-----+----------+\n",
      "| 14|Priya|80000|Civil|         2|\n",
      "+---+-----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.filter(col('Dense_rank') ==2 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+\n",
      "| id|name|  sal|Dept|\n",
      "+---+----+-----+----+\n",
      "| 10|Anil|50000|Auto|\n",
      "+---+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, \"Auto\"),\n",
    "]\n",
    "\n",
    "schema=['id','name','sal','Dept']\n",
    "\n",
    "emp1 = spark.createDataFrame(data=data,schema=schema)\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(sal IS NULL)|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 64850)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "window= Window.orderBy(col('sal').desc())\n",
    "\n",
    "new_df= emp1.withColumn('Dense_rank',dense_rank().over(window))\n",
    "\n",
    "new_df.filter(col('Dense_rank') ==2 ).select(col('sal').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView('one_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|second_Highest_salary|\n",
      "+---------------------+\n",
      "|                 NULL|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT MAX(sal) as second_Highest_salary FROM one_df WHERE sal < (SELECT MAX(sal) FROM one_df);\n",
    "\n",
    " \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "| id| name|  sal|DeptId|\n",
      "+---+-----+-----+------+\n",
      "|  1|  Joe|70000|     1|\n",
      "|  2|  Jim|90000|     1|\n",
      "|  3|Henry|80000|     2|\n",
      "|  4|  Sam|60000|     2|\n",
      "|  5|  Max|90000|     1|\n",
      "+---+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'Joe',70000,1),\n",
    "(2,'Jim',90000,1),\n",
    "(3,'Henry',80000,2),\n",
    "(4,'Sam',60000,2),\n",
    "(5,'Max',90000,1)]\n",
    "\n",
    "schema=['id','name','sal','DeptId']\n",
    "\n",
    "employe_df = spark.createDataFrame(data=data,schema=schema)\n",
    "employe_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|   IT|\n",
      "|  2|Sales|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'IT'),\n",
    "(2,'Sales')\n",
    "]\n",
    "\n",
    "schema=['id','name']\n",
    "\n",
    "dept_df = spark.createDataFrame(data=data,schema=schema)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df= employe_df.alias('emp').join(dept_df.alias('dept'),col('emp.DeptId')==col('dept.id'),'inner').select(col('dept.name').alias('Department'),col('emp.name').alias('Employee'),col('emp.sal'))\n",
    "new_df= employe_df.alias('emp').join(dept_df.alias('dept'),col('emp.DeptId')==col('dept.id'),'inner').select('*',col('dept.name').alias('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+---+-----+----------+\n",
      "| id| name|  sal|DeptId| id| name|Department|\n",
      "+---+-----+-----+------+---+-----+----------+\n",
      "|  1|  Joe|70000|     1|  1|   IT|        IT|\n",
      "|  2|  Jim|90000|     1|  1|   IT|        IT|\n",
      "|  5|  Max|90000|     1|  1|   IT|        IT|\n",
      "|  3|Henry|80000|     2|  2|Sales|     Sales|\n",
      "|  4|  Sam|60000|     2|  2|Sales|     Sales|\n",
      "+---+-----+-----+------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "window= Window.partitionBy('Department').orderBy(col('sal').desc())\n",
    "final_df= new_df.withColumn('dense_rank',dense_rank().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|Department|Employee|  sal|\n",
      "+----------+--------+-----+\n",
      "|        IT|     Jim|90000|\n",
      "|        IT|     Max|90000|\n",
      "|     Sales|   Henry|80000|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.filter(col('dense_rank') ==1 ).drop('dense_rank').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "employe_df.createOrReplaceTempView('employe_df')\n",
    "dept_df.createOrReplaceTempView('dept_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_FEATURE.LATERAL_COLUMN_ALIAS_IN_WINDOW] The feature is not supported: Referencing a lateral column alias `Department` in window expression \"DENSE_RANK() OVER (PARTITION BY lateralAliasReference(Department) ORDER BY salary DESC NULLS LAST unspecifiedframe$())\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43mselect Department.name as Department, Employee.name as Employee,DENSE_RANK() OVER (PARTITION BY Department  ORDER BY  salary DESC ) AS dense_rank\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43m-- From Employee \u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43m-- join Department \u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43m-- on Employee.departmentId = Department.id\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43m-- ORDER BY salary\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43m          \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNSUPPORTED_FEATURE.LATERAL_COLUMN_ALIAS_IN_WINDOW] The feature is not supported: Referencing a lateral column alias `Department` in window expression \"DENSE_RANK() OVER (PARTITION BY lateralAliasReference(Department) ORDER BY salary DESC NULLS LAST unspecifiedframe$())\"."
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select Department.name as Department, Employee.name as Employee,DENSE_RANK() OVER (PARTITION BY Department  ORDER BY  salary DESC ) AS dense_rank\n",
    "-- From Employee emp\n",
    "-- join Department dept\n",
    "-- on Employee.departmentId = Department.id\n",
    "-- ORDER BY salary\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
