{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "# for element in rdd.collect():\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|bonus|\n",
      "+---------------+-----+\n",
      "|    James,Smith|300.0|\n",
      "|      Anna,Rose|410.0|\n",
      "|Robert,Williams|620.0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example 1 mapPartitions()\n",
    "def reformat(partitionData):\n",
    "    for row in partitionData:\n",
    "        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\n",
    "df2=df.rdd.mapPartitions(reformat).toDF([\"name\",\"bonus\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.range(100)\n",
    "#df.show()\n",
    "#df.columns\n",
    "df.select('id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "df1 = df.sample(0.06)\n",
    "print(df1.count())\n",
    "#print(len(df1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 12|\n",
      "| 55|\n",
      "| 62|\n",
      "| 74|\n",
      "| 82|\n",
      "| 89|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+------+\n",
      "|emp_id|    name|dept_id|gender|salary|\n",
      "+------+--------+-------+------+------+\n",
      "|     1|   Smith|     10|  NULL|  3000|\n",
      "|     2|    Rose|     20|     M|  4000|\n",
      "|     3|Williams|     10|     M|  1000|\n",
      "|     4|   Jones|     10|     F|  2000|\n",
      "|     5|   Brown|     30|      |    -1|\n",
      "|     6|   Brown|     30|      |    -1|\n",
      "+------+--------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EMP DataFrame\n",
    "empData = [(1,\"Smith\",10,None,3000),\n",
    "    (2,\"Rose\",20,\"M\",4000),\n",
    "    (3,\"Williams\",10,\"M\",1000),\n",
    "    (4,\"Jones\",10,\"F\",2000),\n",
    "    (5,\"Brown\",30,\"\",-1),\n",
    "    (6,\"Brown\",30,\"\",-1)\n",
    "  ]\n",
    "  \n",
    "empColumns = [\"emp_id\",\"name\",\"dept_id\",\n",
    "  \"gender\",\"salary\"]\n",
    "empDF = spark.createDataFrame(empData,empColumns)\n",
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rows = empDF.count()\n",
    "#print(f\"DataFrame Rows count : {rows}\")\n",
    "empDF.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1.show()\n",
    "df1.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RDD.mapValues() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapValues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mTypeError\u001b[0m: RDD.mapValues() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 1)\n",
      "(32, 1)\n",
      "(42, 2)\n",
      "ParallelCollectionRDD[21] at readRDDFromFile at PythonRDD.scala:289\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(22, 1), (32, 1), (42, 2)])\n",
    "dataColl=rdd.collect()\n",
    "for row in dataColl:\n",
    "    #print(row[0] + \",\" +str(row[1]))\n",
    "    print(row)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, (1, 1))\n",
      "(32, (1, 1))\n",
      "(42, (2, 1))\n"
     ]
    }
   ],
   "source": [
    "#rdd_transformed = rdd.map(lambda x, y: (x[0] + y, x[1] + 1))\n",
    "rdd_transformed = rdd.mapValues(lambda x: (x,1))\n",
    "#rdd_transformed.collect()\n",
    "#print(rdd_transformed)\n",
    "\n",
    "\n",
    "results = rdd_transformed.collect()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x000002352373A8E0>\n"
     ]
    }
   ],
   "source": [
    "seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "print(seqFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 385)\n",
      "(26, 2)\n",
      "(55, 221)\n",
      "(40, 465)\n",
      "(68, 21)\n",
      "(59, 318)\n",
      "(37, 220)\n",
      "(54, 307)\n",
      "(38, 380)\n",
      "(27, 181)\n",
      "(53, 191)\n",
      "(57, 372)\n",
      "(54, 253)\n",
      "(56, 444)\n",
      "(43, 49)\n",
      "(36, 49)\n",
      "(22, 323)\n",
      "(35, 13)\n",
      "(45, 455)\n",
      "(60, 246)\n",
      "(67, 220)\n",
      "(19, 268)\n",
      "(30, 72)\n",
      "(51, 271)\n",
      "(25, 1)\n",
      "(21, 445)\n",
      "(22, 100)\n",
      "(42, 363)\n",
      "(49, 476)\n",
      "(48, 364)\n",
      "(50, 175)\n",
      "(39, 161)\n",
      "(26, 281)\n",
      "(53, 197)\n",
      "(43, 249)\n",
      "(27, 305)\n",
      "(32, 81)\n",
      "(58, 21)\n",
      "(64, 65)\n",
      "(31, 192)\n",
      "(52, 413)\n",
      "(67, 167)\n",
      "(54, 75)\n",
      "(58, 345)\n",
      "(35, 244)\n",
      "(52, 77)\n",
      "(25, 96)\n",
      "(24, 49)\n",
      "(20, 1)\n",
      "(40, 254)\n",
      "(51, 283)\n",
      "(36, 212)\n",
      "(19, 269)\n",
      "(62, 31)\n",
      "(19, 5)\n",
      "(41, 278)\n",
      "(44, 194)\n",
      "(57, 294)\n",
      "(59, 158)\n",
      "(59, 284)\n",
      "(20, 100)\n",
      "(62, 442)\n",
      "(69, 9)\n",
      "(58, 54)\n",
      "(31, 15)\n",
      "(52, 169)\n",
      "(21, 477)\n",
      "(48, 135)\n",
      "(33, 74)\n",
      "(30, 204)\n",
      "(52, 393)\n",
      "(45, 184)\n",
      "(22, 179)\n",
      "(20, 384)\n",
      "(65, 208)\n",
      "(40, 459)\n",
      "(62, 201)\n",
      "(40, 407)\n",
      "(61, 337)\n",
      "(58, 348)\n",
      "(67, 445)\n",
      "(54, 440)\n",
      "(57, 465)\n",
      "(32, 308)\n",
      "(28, 311)\n",
      "(66, 383)\n",
      "(55, 257)\n",
      "(31, 481)\n",
      "(66, 188)\n",
      "(24, 492)\n",
      "(33, 471)\n",
      "(46, 88)\n",
      "(54, 7)\n",
      "(46, 63)\n",
      "(62, 133)\n",
      "(29, 173)\n",
      "(25, 233)\n",
      "(69, 361)\n",
      "(44, 178)\n",
      "(69, 491)\n",
      "(61, 460)\n",
      "(67, 123)\n",
      "(40, 18)\n",
      "(61, 2)\n",
      "(32, 142)\n",
      "(50, 417)\n",
      "(18, 499)\n",
      "(64, 419)\n",
      "(25, 274)\n",
      "(53, 417)\n",
      "(64, 137)\n",
      "(37, 46)\n",
      "(25, 13)\n",
      "(41, 244)\n",
      "(33, 275)\n",
      "(18, 397)\n",
      "(69, 75)\n",
      "(52, 487)\n",
      "(28, 304)\n",
      "(29, 344)\n",
      "(68, 264)\n",
      "(35, 355)\n",
      "(45, 400)\n",
      "(45, 439)\n",
      "(47, 429)\n",
      "(66, 284)\n",
      "(26, 84)\n",
      "(40, 284)\n",
      "(34, 221)\n",
      "(45, 252)\n",
      "(67, 350)\n",
      "(65, 309)\n",
      "(46, 462)\n",
      "(19, 265)\n",
      "(45, 340)\n",
      "(42, 427)\n",
      "(19, 335)\n",
      "(28, 32)\n",
      "(32, 384)\n",
      "(36, 193)\n",
      "(64, 234)\n",
      "(36, 424)\n",
      "(59, 335)\n",
      "(60, 124)\n",
      "(22, 93)\n",
      "(45, 470)\n",
      "(58, 174)\n",
      "(61, 373)\n",
      "(39, 248)\n",
      "(49, 340)\n",
      "(55, 313)\n",
      "(54, 441)\n",
      "(54, 235)\n",
      "(63, 342)\n",
      "(40, 389)\n",
      "(50, 126)\n",
      "(44, 360)\n",
      "(34, 319)\n",
      "(31, 340)\n",
      "(67, 438)\n",
      "(58, 112)\n",
      "(39, 207)\n",
      "(59, 14)\n",
      "(67, 204)\n",
      "(31, 172)\n",
      "(26, 282)\n",
      "(25, 10)\n",
      "(48, 57)\n",
      "(68, 112)\n",
      "(53, 92)\n",
      "(68, 490)\n",
      "(29, 126)\n",
      "(55, 204)\n",
      "(23, 129)\n",
      "(47, 87)\n",
      "(38, 459)\n",
      "(55, 474)\n",
      "(67, 316)\n",
      "(26, 381)\n",
      "(37, 426)\n",
      "(30, 108)\n",
      "(43, 404)\n",
      "(26, 145)\n",
      "(47, 488)\n",
      "(44, 84)\n",
      "(48, 287)\n",
      "(31, 109)\n",
      "(47, 225)\n",
      "(54, 369)\n",
      "(62, 23)\n",
      "(60, 294)\n",
      "(40, 349)\n",
      "(45, 497)\n",
      "(60, 125)\n",
      "(38, 2)\n",
      "(30, 376)\n",
      "(38, 173)\n",
      "(38, 76)\n",
      "(48, 381)\n",
      "(38, 180)\n",
      "(21, 472)\n",
      "(23, 174)\n",
      "(63, 469)\n",
      "(46, 125)\n",
      "(64, 164)\n",
      "(69, 236)\n",
      "(21, 491)\n",
      "(41, 206)\n",
      "(37, 271)\n",
      "(27, 174)\n",
      "(33, 245)\n",
      "(61, 73)\n",
      "(55, 284)\n",
      "(28, 312)\n",
      "(32, 182)\n",
      "(22, 6)\n",
      "(34, 116)\n",
      "(29, 260)\n",
      "(66, 350)\n",
      "(26, 345)\n",
      "(41, 394)\n",
      "(27, 150)\n",
      "(34, 346)\n",
      "(40, 406)\n",
      "(44, 277)\n",
      "(19, 106)\n",
      "(37, 207)\n",
      "(40, 198)\n",
      "(26, 293)\n",
      "(24, 150)\n",
      "(54, 397)\n",
      "(59, 42)\n",
      "(68, 481)\n",
      "(67, 70)\n",
      "(49, 22)\n",
      "(57, 8)\n",
      "(62, 442)\n",
      "(61, 469)\n",
      "(25, 305)\n",
      "(48, 345)\n",
      "(46, 154)\n",
      "(45, 332)\n",
      "(25, 101)\n",
      "(61, 68)\n",
      "(21, 471)\n",
      "(28, 174)\n",
      "(41, 260)\n",
      "(52, 338)\n",
      "(21, 138)\n",
      "(66, 41)\n",
      "(36, 342)\n",
      "(55, 57)\n",
      "(36, 174)\n",
      "(69, 116)\n",
      "(67, 79)\n",
      "(60, 324)\n",
      "(32, 412)\n",
      "(51, 161)\n",
      "(68, 217)\n",
      "(29, 11)\n",
      "(38, 96)\n",
      "(40, 172)\n",
      "(51, 334)\n",
      "(40, 33)\n",
      "(29, 228)\n",
      "(27, 471)\n",
      "(66, 496)\n",
      "(49, 106)\n",
      "(26, 298)\n",
      "(55, 289)\n",
      "(44, 353)\n",
      "(25, 446)\n",
      "(29, 367)\n",
      "(51, 493)\n",
      "(64, 244)\n",
      "(47, 13)\n",
      "(54, 462)\n",
      "(46, 300)\n",
      "(44, 499)\n",
      "(23, 133)\n",
      "(26, 492)\n",
      "(21, 89)\n",
      "(32, 404)\n",
      "(65, 443)\n",
      "(26, 269)\n",
      "(43, 101)\n",
      "(30, 384)\n",
      "(64, 396)\n",
      "(56, 354)\n",
      "(30, 221)\n",
      "(62, 290)\n",
      "(23, 373)\n",
      "(63, 380)\n",
      "(23, 65)\n",
      "(38, 410)\n",
      "(40, 56)\n",
      "(38, 454)\n",
      "(45, 395)\n",
      "(57, 207)\n",
      "(57, 311)\n",
      "(49, 147)\n",
      "(28, 108)\n",
      "(37, 263)\n",
      "(46, 319)\n",
      "(19, 404)\n",
      "(29, 182)\n",
      "(23, 323)\n",
      "(41, 340)\n",
      "(45, 59)\n",
      "(67, 153)\n",
      "(68, 189)\n",
      "(43, 48)\n",
      "(61, 421)\n",
      "(59, 169)\n",
      "(36, 168)\n",
      "(25, 208)\n",
      "(64, 391)\n",
      "(59, 439)\n",
      "(35, 251)\n",
      "(30, 476)\n",
      "(62, 450)\n",
      "(44, 61)\n",
      "(58, 92)\n",
      "(29, 236)\n",
      "(56, 343)\n",
      "(51, 492)\n",
      "(46, 407)\n",
      "(20, 63)\n",
      "(62, 41)\n",
      "(67, 35)\n",
      "(33, 356)\n",
      "(30, 17)\n",
      "(55, 362)\n",
      "(29, 207)\n",
      "(40, 7)\n",
      "(27, 337)\n",
      "(47, 4)\n",
      "(58, 10)\n",
      "(28, 180)\n",
      "(66, 305)\n",
      "(57, 275)\n",
      "(18, 326)\n",
      "(46, 151)\n",
      "(26, 254)\n",
      "(30, 487)\n",
      "(31, 394)\n",
      "(29, 329)\n",
      "(32, 24)\n",
      "(33, 460)\n",
      "(20, 277)\n",
      "(55, 464)\n",
      "(54, 72)\n",
      "(27, 53)\n",
      "(64, 499)\n",
      "(69, 15)\n",
      "(46, 352)\n",
      "(67, 149)\n",
      "(26, 7)\n",
      "(52, 276)\n",
      "(54, 442)\n",
      "(39, 68)\n",
      "(68, 206)\n",
      "(39, 120)\n",
      "(41, 397)\n",
      "(54, 115)\n",
      "(65, 430)\n",
      "(19, 119)\n",
      "(39, 106)\n",
      "(26, 383)\n",
      "(48, 266)\n",
      "(53, 86)\n",
      "(31, 435)\n",
      "(62, 273)\n",
      "(19, 272)\n",
      "(68, 293)\n",
      "(66, 201)\n",
      "(23, 392)\n",
      "(18, 418)\n",
      "(47, 97)\n",
      "(60, 304)\n",
      "(35, 65)\n",
      "(38, 95)\n",
      "(66, 240)\n",
      "(69, 148)\n",
      "(67, 355)\n",
      "(57, 436)\n",
      "(35, 428)\n",
      "(43, 335)\n",
      "(30, 184)\n",
      "(38, 38)\n",
      "(22, 266)\n",
      "(64, 309)\n",
      "(64, 343)\n",
      "(50, 436)\n",
      "(23, 230)\n",
      "(56, 15)\n",
      "(67, 38)\n",
      "(69, 470)\n",
      "(26, 124)\n",
      "(24, 401)\n",
      "(29, 128)\n",
      "(42, 467)\n",
      "(58, 98)\n",
      "(21, 224)\n",
      "(18, 24)\n",
      "(56, 371)\n",
      "(57, 121)\n",
      "(36, 68)\n",
      "(62, 496)\n",
      "(19, 267)\n",
      "(35, 299)\n",
      "(58, 22)\n",
      "(53, 451)\n",
      "(45, 147)\n",
      "(56, 313)\n",
      "(30, 65)\n",
      "(33, 294)\n",
      "(37, 106)\n",
      "(32, 212)\n",
      "(55, 176)\n",
      "(26, 391)\n",
      "(40, 261)\n",
      "(67, 292)\n",
      "(44, 388)\n",
      "(55, 470)\n",
      "(33, 243)\n",
      "(24, 77)\n",
      "(28, 258)\n",
      "(68, 423)\n",
      "(63, 345)\n",
      "(36, 493)\n",
      "(36, 343)\n",
      "(45, 54)\n",
      "(38, 203)\n",
      "(57, 289)\n",
      "(42, 275)\n",
      "(57, 229)\n",
      "(59, 221)\n",
      "(42, 95)\n",
      "(18, 417)\n",
      "(48, 394)\n",
      "(38, 143)\n",
      "(46, 105)\n",
      "(64, 175)\n",
      "(18, 472)\n",
      "(40, 286)\n",
      "(32, 41)\n",
      "(38, 34)\n",
      "(48, 439)\n",
      "(52, 419)\n",
      "(37, 234)\n",
      "(28, 34)\n",
      "(58, 6)\n",
      "(44, 337)\n",
      "(52, 456)\n",
      "(33, 463)\n",
      "(37, 471)\n",
      "(51, 81)\n",
      "(44, 335)\n",
      "(26, 84)\n",
      "(47, 400)\n",
      "(41, 236)\n",
      "(23, 287)\n",
      "(40, 220)\n",
      "(25, 485)\n",
      "(53, 126)\n",
      "(33, 228)\n",
      "(42, 194)\n",
      "(46, 227)\n",
      "(55, 271)\n",
      "(38, 160)\n",
      "(52, 273)\n",
      "(27, 154)\n",
      "(35, 38)\n",
      "(34, 48)\n",
      "(52, 446)\n",
      "(28, 378)\n",
      "(50, 119)\n",
      "(41, 62)\n",
      "(44, 320)\n",
      "(43, 428)\n",
      "(32, 97)\n",
      "(48, 146)\n",
      "(57, 99)\n",
      "(22, 478)\n",
      "(47, 356)\n",
      "(49, 17)\n",
      "(69, 431)\n",
      "(61, 103)\n",
      "(33, 410)\n",
      "(65, 101)\n",
      "(60, 2)\n",
      "(19, 36)\n",
      "(23, 357)\n",
      "(18, 194)\n",
      "(46, 155)\n",
      "(39, 275)\n",
      "(34, 423)\n",
      "(62, 36)\n",
      "(62, 12)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "#sc = SparkContext()\n",
    "lines = spark.sparkContext.textFile(\"C:\\\\Users\\\\10694387\\\\OneDrive - LTIMindtree\\\\Documents\\\\WinUtilsForHadoop\\\\SparkCourse\\\\fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "# print(rdd)\n",
    "\n",
    "results = rdd.collect()\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# totalsByAge = rdd.mapValues(lambda x: (x, 1)).collect()\n",
    "\n",
    "# for result in totalsByAge:\n",
    "#     print(result)\n",
    "\n",
    "\n",
    "# totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "# averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "# results = averagesByAge.collect()\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.repartition(4)\n",
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50278)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lineLengths = distData.map(lambda s: (s+1)).reduce(lambda a, b: a + b)\n",
    "print(lineLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda a, b: a + b)\n",
    "# #results = distData.map(lambda a: a + 1).collect()\n",
    "# resulta = distData.map(lambda s: len(s)).collect()\n",
    "# for result in resulta:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Word\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msg = \"Hello Word\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| Name|Value|\n",
      "+-----+-----+\n",
      "|Mohit|    1|\n",
      "|Rohit|    2|\n",
      "|Virat|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Mohit\",1),(\"Rohit\",2),(\"Virat\",3)],[\"Name\",\"Value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "| Name|Value|index|\n",
      "+-----+-----+-----+\n",
      "|Mohit|    1|    1|\n",
      "|Rohit|    2|    2|\n",
      "|Virat|    3|    3|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m list2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m      3\u001b[0m rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(list1)\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumn1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumn2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[1;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\session.py:1007\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m   1005\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32m~\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\sql\\types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1673\u001b[0m     )\n\u001b[0;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`."
     ]
    }
   ],
   "source": [
    "list1 = [\"a\",\"b\",\"c\",\"d\"]\n",
    "list2 = [1,2,3,4]\n",
    "rdd = spark.sparkContext.parallelize(list1)\n",
    "df = rdd.toDF([\"Column1\", \"Column2\"]) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the lists and convert it to a DataFrame\n",
    "rdd = spark.sparkContext.parallelize(list2)\n",
    "df = rdd.toDF([\"Column1\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "rdd_A = spark.sparkContext.parallelize(list_A)\n",
    "print (rdd_A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "list_B = [4, 5, 6, 7, 8]\n",
    "rdd_B = spark.sparkContext.parallelize(list_B)\n",
    "print (rdd_B.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "result_rdd =rdd_A.subtract(rdd_B)\n",
    "print (result_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentiles\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+\n",
      "|summary|Name|               Age|\n",
      "+-------+----+------------------+\n",
      "|  count|  10|                10|\n",
      "|   mean|NULL|              37.4|\n",
      "| stddev|NULL|22.396428286671068|\n",
      "|    min|   A|                10|\n",
      "|    25%|NULL|                20|\n",
      "|    50%|NULL|                30|\n",
      "|    75%|NULL|                50|\n",
      "|    max|   J|                86|\n",
      "+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"job\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(job='Engineer'), Row(job='Scientist')]\n",
      "Row(job='Engineer')\n",
      "Row(job='Scientist')\n",
      "Engineer\n",
      "Scientist\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Get the top 2 most frequent jobs\n",
    "#top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd\n",
    "#top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).collect()\n",
    "#top_2_jobs = df.groupBy('job').count().collect()\n",
    "print(top_2_jobs.collect())\n",
    "for element in top_2_jobs.collect():\n",
    "    print(element)\n",
    "\n",
    "rdd2=top_2_jobs.flatMap(lambda x: x)\n",
    "for element in rdd2.collect():\n",
    "    print(element)    \n",
    "\n",
    "# Replace all but the top 2 most frequent jobs with 'Other'\n",
    "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
    "\n",
    "# show DataFrame\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1602598246.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[57], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    [Row(job='Engineer', count=4), Row(job='Scientist', count=2), Row(job='Other', count=1)] --> Line 3 o/p\u001b[0m\n\u001b[1;37m                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[Row(job='Engineer', count=4), Row(job='Scientist', count=2), Row(job='Other', count=1)] --> Line 3 o/p\n",
    "[Row(job='Engineer', count=4), Row(job='Scientist', count=2)]  --> line 2 o/p\n",
    "[Row(job='Engineer'), Row(job='Scientist')]  -->line 1 o/p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_2 = df.dropna(subset=['id'])\n",
    "#df_2 = df.dropna(subset=['Values'])\n",
    "df_2 = df.dropna()\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|word_length|\n",
      "+-----+-----------+\n",
      "| john|          4|\n",
      "|alice|          5|\n",
      "|  bob|          3|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('word_length', F.length(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+------+\n",
      "|   name|age|salary|prev_value|  diff|\n",
      "+-------+---+------+----------+------+\n",
      "|  James| 34| 55000|      NULL|     0|\n",
      "|Michael| 30| 70000|     55000| 15000|\n",
      "| Robert| 37| 60000|     70000|-10000|\n",
      "|  Maria| 29| 80000|     60000| 20000|\n",
      "|    Jen| 32| 65000|     80000|-15000|\n",
      "+-------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window specification\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "window = Window.orderBy(\"id\")\n",
    "\n",
    "# Generate the lag of the variable\n",
    "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
    "\n",
    "# Compute the difference with lag\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
    ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example data\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_str_1: string (nullable = true)\n",
      " |-- date_str_2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the pivot operation\n",
    "# pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|series1|series2|\n",
      "+-------+-------+\n",
      "|      1|     10|\n",
      "|      2|      9|\n",
      "|      3|      8|\n",
      "|      4|      7|\n",
      "|      5|      6|\n",
      "|      6|      5|\n",
      "|      7|      4|\n",
      "|      8|      3|\n",
      "|      9|      2|\n",
      "|     10|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your series\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "# missing_count = missing.collect()[0].asDict()\n",
    "# print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n",
      "| Name|Department|Salary|\n",
      "+-----+----------+------+\n",
      "|Rahul|     Sales| 10000|\n",
      "|  Bob|  Finanace| 20000|\n",
      "|Ankit| Marketing| 20000|\n",
      "| Jack| Reporting| 10000|\n",
      "+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Rahul','Sales',10000),('Bob','Finanace',20000),('Ankit','Marketing',20000),('Jack','Reporting',10000)]\n",
    "columns = ['Name','Department','Salary']\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----+\n",
      "| Name|Department|Salary|Bonus|\n",
      "+-----+----------+------+-----+\n",
      "|Rahul|     Sales| 10000|   10|\n",
      "|  Bob|  Finanace| 20000| Null|\n",
      "|Ankit| Marketing| 20000| Null|\n",
      "| Jack| Reporting| 10000|   10|\n",
      "+-----+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.columns\n",
    "df = df.withColumn('Bonus',when(df.Salary < 20000,10).otherwise('Null'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----+------------+\n",
      "| Name|Department|Salary|Bonus|Total Salary|\n",
      "+-----+----------+------+-----+------------+\n",
      "|Rahul|     Sales| 10000|   10|     11000.0|\n",
      "|  Bob|  Finanace| 20000| Null|        NULL|\n",
      "|Ankit| Marketing| 20000| Null|        NULL|\n",
      "| Jack| Reporting| 10000|   10|     11000.0|\n",
      "+-----+----------+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Total Salary',  df.Salary + ((df.Salary * df.Bonus)/100))\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "pyDict = {}\n",
    "\n",
    "point_System = [(['A','E','I','O','U','L','N','V','S','T'],1),\n",
    "                (['D','G'],2),\n",
    "                (['B','C','M','P'],3),\n",
    "                (['F','H','B','W','Y'],4),\n",
    "                (['K'],5),\n",
    "                (['J','X'],8),\n",
    "                (['G','Z'],10)]\n",
    "\n",
    "for i,j in point_System:\n",
    "    for ii in i:\n",
    "        pyDict.update({ii:j})\n",
    "    #print(i)\n",
    "    #print(j)\n",
    "        \n",
    "#print(pyDict)\n",
    "\n",
    "name = 'MOHIT'\n",
    "total = 0\n",
    "for a in name:\n",
    "    val = pyDict.get(a)\n",
    "    total= total + val\n",
    "\n",
    "print(total)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 43, 56, 33, 12, 59, 345]\n",
      "<built-in method sort of list object at 0x0000020C902B1E80>\n"
     ]
    }
   ],
   "source": [
    "a = [4,8,43,56,33,12,59,345]\n",
    "print(a)\n",
    "\n",
    "print(a.sort())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# val = input(\"Enter the value\")\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 12, 33, 43, 56, 59, 345]\n",
      "Position 7\n",
      "[4, 8, 12, 33, 43, 56, 59, 67, 345]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53910)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "a = [4,8,43,56,33,12,59,345]\n",
    "#print(a)\n",
    "a.sort()\n",
    "print(a)\n",
    "b = 67\n",
    "position = 0\n",
    "for i in a:\n",
    "    # print(\"I is \", i)\n",
    "    # print(\" Index of i is\",a.index(i))\n",
    "    if(i > b ):\n",
    "       position = a.index(i)\n",
    "       print(\"Position\", position)\n",
    "\n",
    "a.insert(position,b)        \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+------+-----------+\n",
      "|Id |Name    |Salary|Department|Gender|TotalSalary|\n",
      "+---+--------+------+----------+------+-----------+\n",
      "|1  |manish  |50000 |IT        |m     |345000     |\n",
      "|4  |mukesh  |80000 |IT        |m     |345000     |\n",
      "|8  |rashi   |100000|IT        |f     |345000     |\n",
      "|9  |aditya  |65000 |IT        |m     |345000     |\n",
      "|11 |rakhi   |50000 |IT        |f     |345000     |\n",
      "|3  |raushan |70000 |marketing |m     |220000     |\n",
      "|6  |nikita  |45000 |marketing |f     |220000     |\n",
      "|7  |ragini  |55000 |marketing |f     |220000     |\n",
      "|10 |rahul   |50000 |marketing |m     |220000     |\n",
      "|2  |vikash  |60000 |sales     |m     |240000     |\n",
      "|5  |priti   |90000 |sales     |f     |240000     |\n",
      "|12 |akhilesh|90000 |sales     |m     |240000     |\n",
      "+---+--------+------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "columns = ['Id','Name','Salary','Department','Gender']\n",
    "\n",
    "# emp_df = spark.createDataframe(data= emp_data,schema = columns)\n",
    "df = spark.createDataFrame(data=emp_data, schema = columns)\n",
    "\n",
    "# print(df.groupBy('Department').count().show())\n",
    "# print(df.groupBy('Department').agg(sum('Salary')).show())\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "window = Window.partitionBy(col('Department'))\n",
    "\n",
    "emp_df = df.withColumn('TotalSalary',sum(col('Salary')).over(window)) \\\n",
    "         .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Window [sum(Salary#632L) windowspecdefinition(Department#633, Salary#632L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS TotalSalary#743L], [Department#633], [Salary#632L ASC NULLS FIRST]\n",
      "   +- Sort [Department#633 ASC NULLS FIRST, Salary#632L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(Department#633, 200), ENSURE_REQUIREMENTS, [plan_id=998]\n",
      "         +- Scan ExistingRDD[Id#630L,Name#631,Salary#632L,Department#633,Gender#634]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 62720)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\10694387\\OneDrive - LTIMindtree\\Documents\\WinUtilsForHadoop\\spark-3.5.0\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\10694387\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# window = Window.partitionBy(col('Department')).orderBy(col('Salary')).rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "window = Window.partitionBy(col('Department')).orderBy(col('Salary'))\n",
    "\n",
    "emp_df = df.withColumn('TotalSalary',sum(col('Salary')).over(window)) \n",
    "# emp_df = df.withColumn('TotalDepartmentSalary',row_number().over(window))\n",
    "\n",
    "emp_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+\n",
      "| Id|   Name|      Date|  Sales|\n",
      "+---+-------+----------+-------+\n",
      "|  1| iphone|01-01-2023|1500000|\n",
      "|  2|samsung|01-01-2023|1100000|\n",
      "|  3|oneplus|01-01-2023|1100000|\n",
      "|  1| iphone|01-02-2023|1300000|\n",
      "|  2|samsung|01-02-2023|1120000|\n",
      "|  3|oneplus|01-02-2023|1120000|\n",
      "|  1| iphone|01-03-2023|1600000|\n",
      "|  2|samsung|01-03-2023|1080000|\n",
      "|  3|oneplus|01-03-2023|1160000|\n",
      "|  1| iphone|01-04-2023|1700000|\n",
      "|  2|samsung|01-04-2023|1800000|\n",
      "|  3|oneplus|01-04-2023|1170000|\n",
      "|  1| iphone|01-05-2023|1200000|\n",
      "|  2|samsung|01-05-2023| 980000|\n",
      "|  3|oneplus|01-05-2023|1175000|\n",
      "|  1| iphone|01-06-2023|1100000|\n",
      "|  2|samsung|01-06-2023|1100000|\n",
      "|  3|oneplus|01-06-2023|1200000|\n",
      "+---+-------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "columns = ['Id','Name','Date','Sales']\n",
    "\n",
    "# emp_df = spark.createDataframe(data= emp_data,schema = columns)\n",
    "df = spark.createDataFrame(data=product_data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+------------------+\n",
      "| Id|   Name|      Date|  Sales|PreviousMonthSales|\n",
      "+---+-------+----------+-------+------------------+\n",
      "|  1| iphone|01-06-2023|1100000|              NULL|\n",
      "|  1| iphone|01-05-2023|1200000|           1100000|\n",
      "|  1| iphone|01-02-2023|1300000|           1200000|\n",
      "|  1| iphone|01-01-2023|1500000|           1300000|\n",
      "|  1| iphone|01-03-2023|1600000|           1500000|\n",
      "|  1| iphone|01-04-2023|1700000|           1600000|\n",
      "|  3|oneplus|01-01-2023|1100000|              NULL|\n",
      "|  3|oneplus|01-02-2023|1120000|           1100000|\n",
      "|  3|oneplus|01-03-2023|1160000|           1120000|\n",
      "|  3|oneplus|01-04-2023|1170000|           1160000|\n",
      "|  3|oneplus|01-05-2023|1175000|           1170000|\n",
      "|  3|oneplus|01-06-2023|1200000|           1175000|\n",
      "|  2|samsung|01-05-2023| 980000|              NULL|\n",
      "|  2|samsung|01-03-2023|1080000|            980000|\n",
      "|  2|samsung|01-01-2023|1100000|           1080000|\n",
      "|  2|samsung|01-06-2023|1100000|           1100000|\n",
      "|  2|samsung|01-02-2023|1120000|           1100000|\n",
      "|  2|samsung|01-04-2023|1800000|           1120000|\n",
      "+---+-------+----------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('Name').orderBy('Sales')\n",
    "\n",
    "df = df.withColumn('PreviousMonthSales',lag('Sales',1).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+------------------+------------------+\n",
      "| Id|   Name|      Date|  Sales|PreviousMonthSales|IncreaseOrDecrease|\n",
      "+---+-------+----------+-------+------------------+------------------+\n",
      "|  1| iphone|01-06-2023|1100000|              NULL|              NULL|\n",
      "|  1| iphone|01-05-2023|1200000|           1100000|            100000|\n",
      "|  1| iphone|01-02-2023|1300000|           1200000|            100000|\n",
      "|  1| iphone|01-01-2023|1500000|           1300000|            200000|\n",
      "|  1| iphone|01-03-2023|1600000|           1500000|            100000|\n",
      "|  1| iphone|01-04-2023|1700000|           1600000|            100000|\n",
      "|  3|oneplus|01-01-2023|1100000|              NULL|              NULL|\n",
      "|  3|oneplus|01-02-2023|1120000|           1100000|             20000|\n",
      "|  3|oneplus|01-03-2023|1160000|           1120000|             40000|\n",
      "|  3|oneplus|01-04-2023|1170000|           1160000|             10000|\n",
      "|  3|oneplus|01-05-2023|1175000|           1170000|              5000|\n",
      "|  3|oneplus|01-06-2023|1200000|           1175000|             25000|\n",
      "|  2|samsung|01-05-2023| 980000|              NULL|              NULL|\n",
      "|  2|samsung|01-03-2023|1080000|            980000|            100000|\n",
      "|  2|samsung|01-01-2023|1100000|           1080000|             20000|\n",
      "|  2|samsung|01-06-2023|1100000|           1100000|                 0|\n",
      "|  2|samsung|01-02-2023|1120000|           1100000|             20000|\n",
      "|  2|samsung|01-04-2023|1800000|           1120000|            680000|\n",
      "+---+-------+----------+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('IncreaseOrDecrease',col('Sales') - col('PreviousMonthSales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the avg sal of managers, there are multiple employees can work under the manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','sal','mngr_id']\n",
    "\n",
    "manager_df= spark.createDataFrame(data=data,schema=schema)\n",
    "manager_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_df_new= manager_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|mngr_id|avg(sal)|\n",
      "+-------+--------+\n",
      "|     18| 53750.0|\n",
      "|     16| 65000.0|\n",
      "|     17| 62500.0|\n",
      "|     10| 90000.0|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+------+--------+\n",
      "|mngr_id|  name|avg(sal)|\n",
      "+-------+------+--------+\n",
      "|     10|  Anil| 90000.0|\n",
      "|     16|Rajesh| 65000.0|\n",
      "|     17| Raman| 62500.0|\n",
      "|     18|   Sam| 53750.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1 = manager_df.groupBy('mngr_id').avg('sal')\n",
    "\n",
    "# df1.printSchema()\n",
    "df1.show()\n",
    "df1.alias(\"emp1\").join(manager_df.alias(\"emp2\"),col(\"emp1.mngr_id\")==col(\"emp2.id\"),'inner').select(col(\"emp1.mngr_id\"),col(\"emp2.name\"),col(\"emp1.avg(sal)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager_df.createOrReplaceTempView(\"manager_tbl\")\n",
    "spark.sql(\"\"\"\n",
    "select * from manager_tbl\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from manager_tbl\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the employees whoes salary is greater than their manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','sal','mngr_id']\n",
    "\n",
    "emp_df= spark.createDataFrame(data=data,schema=schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_new = emp_df.groupBy('mngr_id','sal').count()\n",
    "df_new = emp_df\n",
    "df_new.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+---------+\n",
      "| id|  name|  sal|mngr_id|mngr_salr|\n",
      "+---+------+-----+-------+---------+\n",
      "| 10|  Anil|50000|     18|    65000|\n",
      "| 11| Vikas|75000|     16|    90000|\n",
      "| 12| Nisha|40000|     18|    65000|\n",
      "| 13| Nidhi|60000|     17|    55000|\n",
      "| 14| Priya|80000|     18|    65000|\n",
      "| 15| Mohit|45000|     18|    65000|\n",
      "| 16|Rajesh|90000|     10|    50000|\n",
      "| 18|   Sam|65000|     17|    55000|\n",
      "| 17| Raman|55000|     16|    90000|\n",
      "+---+------+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff =df_new.alias('emp1').join(emp_df.alias('emp2'),col('emp1.id')== col('emp2.mngr_id'),\"right\").select(col('emp2.id'),col('emp2.name'),col('emp2.sal'),col('emp2.mngr_id'),col('emp1.sal').alias('mngr_salr')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+---------+\n",
      "| id|  name|  sal|mngr_id|mngr_salr|\n",
      "+---+------+-----+-------+---------+\n",
      "| 16|Rajesh|90000|     10|    50000|\n",
      "| 13| Nidhi|60000|     17|    55000|\n",
      "| 18|   Sam|65000|     17|    55000|\n",
      "| 14| Priya|80000|     18|    65000|\n",
      "+---+------+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.filter(col('sal')>col('mngr_salr')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('emp_df')\n",
    "df_new.createOrReplaceTempView('df_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+---------+\n",
      "| id|  name|  sal|mngr_id|mngr_salr|\n",
      "+---+------+-----+-------+---------+\n",
      "| 16|Rajesh|90000|     10|    50000|\n",
      "| 13| Nidhi|60000|     17|    55000|\n",
      "| 18|   Sam|65000|     17|    55000|\n",
      "| 14| Priya|80000|     18|    65000|\n",
      "+---+------+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          \n",
    "          select * from \n",
    "          (\n",
    "            select emp2.id,emp2.name,emp2.sal,emp2.mngr_id,emp1.sal as mngr_salr from df_new as emp1 inner join emp_df as emp2\n",
    "            on emp1.id == emp2.mngr_id\n",
    "          )\n",
    "          where sal > mngr_salr\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMAZONE Interview Questions - find the sal of employees whoes sal is greater than avg sal of dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|EmpIdd|DeptId|\n",
      "+------+------+\n",
      "|    10|     3|\n",
      "|    11|     2|\n",
      "|    12|     2|\n",
      "|    13|     3|\n",
      "|    14|     1|\n",
      "|    15|     1|\n",
      "|    16|     2|\n",
      "|    17|     4|\n",
      "|    18|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10,3 ),\n",
    "(11 ,2),\n",
    "(12 ,2),\n",
    "(13 ,3),\n",
    "(14 ,1),\n",
    "(15 ,1),\n",
    "(16 ,2),\n",
    "(17 ,4),\n",
    "(18 ,4)]\n",
    "\n",
    "schema=['EmpIdd','DeptId']\n",
    "\n",
    "dept_df= spark.createDataFrame(data=data,schema=schema)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= dept_df.join(emp_df, dept_df.EmpIdd == emp_df.id,'inner').select('id','name','sal','mngr_id','DeptId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|DeptId|         avg(sal)|\n",
      "+------+-----------------+\n",
      "|     1|          62500.0|\n",
      "|     3|          55000.0|\n",
      "|     2|68333.33333333333|\n",
      "|     4|          60000.0|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.groupBy('DeptId').avg('sal')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.alias('emp1').join(df3.alias('emp2'), col('emp2.DeptId') == col('emp1.DeptId'),'inner').select(col('emp2.id'),col('emp2.name'),col('emp2.sal'),col('emp2.mngr_id'),col('emp2.DeptId'),col('emp1.avg(sal)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+------+-----------------+\n",
      "| id|  name|  sal|mngr_id|DeptId|         avg(sal)|\n",
      "+---+------+-----+-------+------+-----------------+\n",
      "| 11| Vikas|75000|     16|     2|68333.33333333333|\n",
      "| 13| Nidhi|60000|     17|     3|          55000.0|\n",
      "| 14| Priya|80000|     18|     1|          62500.0|\n",
      "| 16|Rajesh|90000|     10|     2|68333.33333333333|\n",
      "| 18|   Sam|65000|     17|     4|          60000.0|\n",
      "+---+------+-----+-------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.filter(col('sal')>col('avg(sal)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|countryId|countryName|\n",
      "+---------+-----------+\n",
      "|        1|      india|\n",
      "|        2|         US|\n",
      "|        3|  Australia|\n",
      "|        4|      Japan|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1 ,'india'),\n",
    "(2 ,'US'),\n",
    "(3 ,'Australia'),\n",
    "(4 ,'Japan'),\n",
    "]\n",
    "\n",
    "schema=['countryId','countryName']\n",
    "\n",
    "country_df= spark.createDataFrame(data=data,schema=schema)\n",
    "country_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "|cId|cityId|popullation|\n",
      "+---+------+-----------+\n",
      "|  1|     1|      50000|\n",
      "|  1|     2|      75000|\n",
      "|  1|     3|      40000|\n",
      "|  2|     1|      60000|\n",
      "|  3|     1|      80000|\n",
      "|  2|     2|      45000|\n",
      "|  3|     2|      90000|\n",
      "|  4|     1|      55000|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1 ,1,50000),\n",
    "(1 ,2,75000),\n",
    "(1 ,3,40000),\n",
    "(2 ,1,60000),\n",
    "(3 ,1,80000),\n",
    "(2 ,2,45000),\n",
    "(3 ,2,90000),\n",
    "(4 ,1,55000),\n",
    "]\n",
    "\n",
    "schema=['cId','cityId','popullation']\n",
    "\n",
    "pop_df= spark.createDataFrame(data=data,schema=schema)\n",
    "pop_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = country_df.alias('emp1').join(pop_df.alias('emp2'),col('emp1.countryId') == col('emp2.cId'),'inner').select(col('emp2.cId'),col('emp2.popullation'),col('emp1.countryName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "|cId|popullation|countryName|\n",
      "+---+-----------+-----------+\n",
      "|  1|      50000|      india|\n",
      "|  1|      75000|      india|\n",
      "|  1|      40000|      india|\n",
      "|  2|      60000|         US|\n",
      "|  2|      45000|         US|\n",
      "|  3|      80000|  Australia|\n",
      "|  3|      90000|  Australia|\n",
      "|  4|      55000|      Japan|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+------+------+\n",
      "|cId|Australia|Japan|    US| india|\n",
      "+---+---------+-----+------+------+\n",
      "|  1|     NULL| NULL|  NULL|165000|\n",
      "|  2|     NULL| NULL|105000|  NULL|\n",
      "|  3|   170000| NULL|  NULL|  NULL|\n",
      "|  4|     NULL|55000|  NULL|  NULL|\n",
      "+---+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('cId').pivot('countryName').sum('popullation').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupBy('countryName').sum('popullation').withColumnRenamed(\"sum('popullation')\", \"pop\")\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|countryName|sum(popullation)|\n",
      "+-----------+----------------+\n",
      "|      india|          165000|\n",
      "|         US|          105000|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows with id less than 4 \n",
    "df3.where((col('countryName') == 'india') | (col('countryName') == 'US')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.where((col('countryName') == 'india') | (col('countryName') == 'US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|countryName|sum(popullation)|\n",
      "+-----------+----------------+\n",
      "|      india|          165000|\n",
      "|         US|          105000|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 =df4.groupBy('countryName').pivot('countryName').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|countryName|    US| india|\n",
      "+-----------+------+------+\n",
      "|      india|  NULL|165000|\n",
      "|         US|105000|  NULL|\n",
      "+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.drop('countryName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    US| india|\n",
      "+------+------+\n",
      "|     0|165000|\n",
      "|105000|     0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sum() on multiple columns\n",
    "from pyspark.sql.functions import sum\n",
    "df7 = df6.select(sum(df6.US).alias(\"US\"), \n",
    "          sum(df6.india).alias(\"INDIA\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+\n",
      "|    US| INDIA|TotalPop|\n",
      "+------+------+--------+\n",
      "|105000|165000|  270000|\n",
      "+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = df7.withColumn('TotalPop', df7.US + df7.INDIA).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+\n",
      "|countryId|cityId|popullation|\n",
      "+---------+------+-----------+\n",
      "|        1|     1|      50000|\n",
      "|        1|     2|      75000|\n",
      "|        1|     3|      40000|\n",
      "|        2|     1|      60000|\n",
      "|        3|     1|      80000|\n",
      "|        2|     2|      45000|\n",
      "|        3|     2|      90000|\n",
      "|        4|     1|      55000|\n",
      "+---------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1 ,1,50000),\n",
    "(1 ,2,75000),\n",
    "(1 ,3,40000),\n",
    "(2 ,1,60000),\n",
    "(3 ,1,80000),\n",
    "(2 ,2,45000),\n",
    "(3 ,2,90000),\n",
    "(4 ,1,55000),\n",
    "]\n",
    "\n",
    "schema=['countryId','cityId','popullation']\n",
    "\n",
    "pop_df= spark.createDataFrame(data=data,schema=schema)\n",
    "pop_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|countryId|countryName|\n",
      "+---------+-----------+\n",
      "|        1|      india|\n",
      "|        2|         US|\n",
      "|        3|  Australia|\n",
      "|        4|      Japan|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1 ,'india'),\n",
    "(2 ,'US'),\n",
    "(3 ,'Australia'),\n",
    "(4 ,'Japan'),\n",
    "]\n",
    "\n",
    "schema=['countryId','countryName']\n",
    "\n",
    "country_df= spark.createDataFrame(data=data,schema=schema)\n",
    "country_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.createOrReplaceTempView('pop_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df.createOrReplaceTempView('country_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+-----------+\n",
      "|countryId|cityId|popullation|countryName|\n",
      "+---------+------+-----------+-----------+\n",
      "|        1|     1|      50000|      india|\n",
      "|        1|     2|      75000|      india|\n",
      "|        1|     3|      40000|      india|\n",
      "|        2|     1|      60000|         US|\n",
      "|        2|     2|      45000|         US|\n",
      "|        3|     1|      80000|  Australia|\n",
      "|        3|     2|      90000|  Australia|\n",
      "|        4|     1|      55000|      Japan|\n",
      "+---------+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"    \n",
    "           \n",
    "          select pt.countryId,pt.cityId,pt.popullation,ct.countryName from pop_tble as pt \n",
    "        inner join country_tble as ct \n",
    "        on pt.countryId = ct.countryId\n",
    "         \"\"\").show()\n",
    "\n",
    "\n",
    "# df2 = country_df.alias('emp1').join(pop_df.alias('emp2'),col('emp1.countryId') == col('emp2.cId'),'inner').select(col('emp2.cId'),col('emp2.popullation'),col('emp1.countryName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|countryName|popSum|\n",
      "+-----------+------+\n",
      "|      india|165000|\n",
      "|         US|105000|\n",
      "|      Japan| 55000|\n",
      "|  Australia|170000|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df1 = spark.sql(\"\"\"\n",
    "          \n",
    "        select countryName,sum(popullation) as popSum from\n",
    "        (select pt.countryId,pt.cityId,pt.popullation,ct.countryName from pop_tble as pt \n",
    "        inner join country_tble as ct \n",
    "        on pt.countryId = ct.countryId )    \n",
    "        GROUP BY countryName  \n",
    "         \n",
    "         \"\"\")\n",
    "pop_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df1.createOrReplaceTempView('pop_df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "| INDIA|   USA|(INDIA + USA)|\n",
      "+------+------+-------------+\n",
      "|165000|105000|       270000|\n",
      "+------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          select INDIA,USA, INDIA + USA from (\n",
    "          select sum(INDIA) as INDIA,SUM(USA) as USA FROM \n",
    "          (\n",
    "          select \n",
    "          case when countryName == 'india' then popSum else 0 end as INDIA,\n",
    "          case when countryName == 'US' then popSum else 0 end as USA          \n",
    "          FROM pop_df1))\n",
    "         \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  NULL|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  NULL|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|new_gender|\n",
      "+----------+\n",
      "|      Male|\n",
      "|      Male|\n",
      "|          |\n",
      "|    Female|\n",
      "|          |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(when(df.gender == \"M\",\"Male\")\n",
    "                  .when(df.gender == \"F\",\"Female\")\n",
    "                  .when(df.gender.isNull() ,\"\")\n",
    "                  .otherwise(df.gender).alias(\"new_gender\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|countryName|popSum|\n",
      "+-----------+------+\n",
      "|      india|165000|\n",
      "|         US|105000|\n",
      "|      Japan| 55000|\n",
      "|  Australia|170000|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| INDIA|   USA|\n",
      "+------+------+\n",
      "|165000|     0|\n",
      "|     0|105000|\n",
      "|     0|     0|\n",
      "|     0|     0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df2 = pop_df1.select(when(pop_df1.countryName == 'india',pop_df1.popSum).otherwise(0).alias('INDIA')\n",
    "               ,when(pop_df1.countryName == 'US',pop_df1.popSum).otherwise(0).alias('USA') )\n",
    "\n",
    "pop_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| INDIA|   USA|\n",
      "+------+------+\n",
      "|165000|105000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_df3=pop_df2.select( sum(col('INDIA')).alias('INDIA'),sum(col('USA')).alias('USA'))\n",
    "pop_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------------------+\n",
      "| INDIA|   USA|(USA + INDIA AS total)|\n",
      "+------+------+----------------------+\n",
      "|165000|105000|                270000|\n",
      "+------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# pop_df3.withColumns('TotalSum', sum(pop_df3[col] for col in pop_df3.columns)).collect()\n",
    "# pop_df3.withColumns('TotalSum', expr(\"pop_df3.INDIA + pop_df3.USA\")).show()\n",
    "pop_df3.select('*', (col(\"USA\")+col(\"INDIA\").alias(\"total\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','sal','mngr_id']\n",
    "\n",
    "emp_df= spark.createDataFrame(data=data,schema=schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|EmpIdd|DeptId|\n",
      "+------+------+\n",
      "|    10|     3|\n",
      "|    11|     2|\n",
      "|    12|     2|\n",
      "|    13|     3|\n",
      "|    14|     1|\n",
      "|    15|     1|\n",
      "|    16|     2|\n",
      "|    17|     4|\n",
      "|    18|     4|\n",
      "+------+------+\n",
      "\n",
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show()\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'sessionState'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[337], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msessionState\u001b[49m\u001b[38;5;241m.\u001b[39mexecutePlan(emp_df\u001b[38;5;241m.\u001b[39mqueryExecution\u001b[38;5;241m.\u001b[39mlogical)\u001b[38;5;241m.\u001b[39moptimizedPlan\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39msizeInBytes) \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'sessionState'"
     ]
    }
   ],
   "source": [
    "print(spark.sessionState.executePlan(emp_df.queryExecution.logical).optimizedPlan.stats.sizeInBytes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
